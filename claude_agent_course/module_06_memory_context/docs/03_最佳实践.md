# 03. æœ€ä½³å®è·µ - ç”Ÿäº§ç¯å¢ƒæŒ‡å—

> ä»åŸå‹åˆ°ç”Ÿäº§ï¼šè®°å¿†ç³»ç»Ÿçš„å·¥ç¨‹åŒ–å®è·µ

## ğŸ¯ æœ¬ç« ç›®æ ‡

- æŒæ¡ç”Ÿäº§çº§è®°å¿†ç³»ç»Ÿè®¾è®¡
- å­¦ä¹ æˆæœ¬ä¼˜åŒ–ç­–ç•¥
- äº†è§£æ€§èƒ½è°ƒä¼˜æŠ€å·§
- å»ºç«‹ç›‘æ§å’Œæ•…éšœå¤„ç†æœºåˆ¶

---

## 1. ç³»ç»Ÿæ¶æ„è®¾è®¡

### 1.1 åˆ†å±‚æ¶æ„

```mermaid
graph TB
    subgraph "åº”ç”¨å±‚"
        A[Agent Interface]
    end
    
    subgraph "è®°å¿†å±‚"
        B[Session Manager]
        C[Cache Controller]
        D[Vector Store]
    end
    
    subgraph "å­˜å‚¨å±‚"
        E[JSON/SQLite]
        F[Redis Cache]
        G[Vector DB]
    end
    
    A --> B
    A --> C
    A --> D
    B --> E
    C --> F
    D --> G
```

**è®¾è®¡åŸåˆ™**ï¼š
- ğŸ”Œ **è§£è€¦**ï¼šå„ç»„ä»¶ç‹¬ç«‹ï¼Œæ˜“äºæ›¿æ¢
- ğŸ“Š **å¯è§‚æµ‹**ï¼šå®Œå–„çš„æ—¥å¿—å’Œç›‘æ§
- ğŸ”„ **å¯æ‰©å±•**ï¼šæ”¯æŒæ°´å¹³æ‰©å±•
- ğŸ›¡ï¸ **å®¹é”™**ï¼šä¼˜é›…å¤„ç†æ•…éšœ

### 1.2 é€‰å‹æŒ‡å—

#### Session å­˜å‚¨é€‰å‹

| æ–¹æ¡ˆ | QPS | æˆæœ¬ | é€‚ç”¨è§„æ¨¡ | æ¨èåœºæ™¯ |
|------|-----|------|---------|---------|
| **JSON æ–‡ä»¶** | < 10 | å…è´¹ | < 1K ç”¨æˆ· | åŸå‹ã€å­¦ä¹  |
| **SQLite** | < 100 | å…è´¹ | < 10K ç”¨æˆ· | ä¸­å°åº”ç”¨ |
| **Redis** | > 10K | ä¸­ç­‰ | < 1M ç”¨æˆ· | é«˜å¹¶å‘åº”ç”¨ |
| **PostgreSQL** | > 1K | ä¸­ç­‰ | æ— é™åˆ¶ | ä¼ä¸šçº§åº”ç”¨ |

#### å‘é‡æ•°æ®åº“é€‰å‹

| æ•°æ®åº“ | æ€§èƒ½ | æ˜“ç”¨æ€§ | æˆæœ¬ | é€‚ç”¨åœºæ™¯ |
|--------|------|--------|------|---------|
| **ChromaDB** | â­â­â­ | â­â­â­â­â­ | å…è´¹ | åŸå‹å¼€å‘ |
| **FAISS** | â­â­â­â­â­ | â­â­â­ | å…è´¹ | æœ¬åœ°é«˜æ€§èƒ½ |
| **Pinecone** | â­â­â­â­ | â­â­â­â­â­ | ä»˜è´¹ | äº‘æœåŠ¡ |
| **Qdrant** | â­â­â­â­ | â­â­â­â­ | å…è´¹/ä»˜è´¹ | ç”Ÿäº§ç¯å¢ƒ |
| **Weaviate** | â­â­â­â­ | â­â­â­â­ | å…è´¹/ä»˜è´¹ | å¤šæ¨¡æ€ |

### 1.3 æ··åˆæ¶æ„ç¤ºä¾‹

```python
class ProductionMemorySystem:
    """ç”Ÿäº§çº§è®°å¿†ç³»ç»Ÿ"""
    
    def __init__(self, config: Dict):
        # çŸ­æœŸè®°å¿†ï¼šRedisï¼ˆå¿«é€Ÿè®¿é—®ï¼‰
        self.redis_client = redis.Redis(
            host=config['redis_host'],
            port=config['redis_port'],
            db=0,
            decode_responses=True
        )
        
        # é•¿æœŸè®°å¿†ï¼šPostgreSQLï¼ˆæŒä¹…åŒ–ï¼‰
        self.db = psycopg2.connect(config['database_url'])
        
        # è¯­ä¹‰è®°å¿†ï¼šå‘é‡æ•°æ®åº“ï¼ˆç›¸ä¼¼åº¦æœç´¢ï¼‰
        self.vector_db = QdrantClient(
            host=config['qdrant_host'],
            port=config['qdrant_port']
        )
    
    def get_context(self, session_id: str, query: str) -> Dict:
        """æ™ºèƒ½è·å–ä¸Šä¸‹æ–‡"""
        context = {}
        
        # 1. çŸ­æœŸè®°å¿†ï¼ˆæœ€è¿‘å¯¹è¯ï¼‰- Redis
        recent_messages = self._get_recent_from_redis(session_id, limit=10)
        context['recent'] = recent_messages
        
        # 2. é•¿æœŸè®°å¿†ï¼ˆé‡è¦å†å²ï¼‰- PostgreSQL
        important_facts = self._get_important_from_db(session_id)
        context['facts'] = important_facts
        
        # 3. è¯­ä¹‰è®°å¿†ï¼ˆç›¸å…³çŸ¥è¯†ï¼‰- Vector DB
        relevant_docs = self._semantic_search(query, top_k=3)
        context['knowledge'] = relevant_docs
        
        return context
    
    def save_message(self, session_id: str, message: Dict):
        """ä¿å­˜æ¶ˆæ¯åˆ°å¤šä¸ªå­˜å‚¨"""
        # 1. ç«‹å³å†™å…¥ Redisï¼ˆå¿«é€Ÿè®¿é—®ï¼‰
        self._save_to_redis(session_id, message, ttl=3600)
        
        # 2. å¼‚æ­¥å†™å…¥ PostgreSQLï¼ˆæŒä¹…åŒ–ï¼‰
        self._async_save_to_db(session_id, message)
        
        # 3. å¦‚æœæ˜¯é‡è¦ä¿¡æ¯ï¼Œå‘é‡åŒ–å­˜å‚¨
        if self._is_important(message):
            self._save_to_vector_db(session_id, message)
```

---

## 2. æˆæœ¬ä¼˜åŒ–ç­–ç•¥

### 2.1 Prompt Caching æœ€å¤§åŒ–

**è§„åˆ™ 1ï¼šç¼“å­˜å¤§å—å›ºå®šå†…å®¹**

```python
# âŒ ä¸å¥½ï¼šé¢‘ç¹å˜åŒ–çš„å†…å®¹
messages = [
    {
        "role": "user",
        "content": f"å½“å‰æ—¶é—´: {datetime.now()}, ç”¨æˆ·: {user_name}"  # æ¯æ¬¡éƒ½å˜
    }
]

# âœ… å¥½ï¼šå›ºå®šå†…å®¹ç¼“å­˜ï¼ŒåŠ¨æ€å†…å®¹åˆ†ç¦»
messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "text",
                "text": LARGE_SYSTEM_PROMPT,  # å›ºå®šï¼Œå¯ç¼“å­˜
                "cache_control": {"type": "ephemeral"}
            },
            {
                "type": "text",
                "text": f"å½“å‰æ—¶é—´: {datetime.now()}, ç”¨æˆ·: {user_name}"  # åŠ¨æ€
            }
        ]
    }
]
```

**è§„åˆ™ 2ï¼šåˆå¹¶ç¼“å­˜å—**

```python
# âŒ ä¸å¥½ï¼šå¤šä¸ªå°ç¼“å­˜å—
cache_block_1 = "æ–‡æ¡£ç¬¬ 1 éƒ¨åˆ†"  # 500 tokens
cache_block_2 = "æ–‡æ¡£ç¬¬ 2 éƒ¨åˆ†"  # 600 tokens

# âœ… å¥½ï¼šå•ä¸ªå¤§ç¼“å­˜å—ï¼ˆ> 1024 tokensï¼‰
combined_doc = f"{part1}\n\n{part2}\n\n{part3}"  # 3000 tokens
```

**è§„åˆ™ 3ï¼šç¼“å­˜ç”Ÿå‘½å‘¨æœŸç®¡ç†**

```python
class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self):
        self.cache_tracker = {}
    
    def should_refresh(self, cache_key: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ·æ–°ç¼“å­˜"""
        if cache_key not in self.cache_tracker:
            return True
        
        last_update = self.cache_tracker[cache_key]
        # TTL = 5 åˆ†é’Ÿï¼Œ4 åˆ†é’Ÿååˆ·æ–°
        return (datetime.now() - last_update).seconds > 240
    
    def mark_cached(self, cache_key: str):
        """æ ‡è®°ç¼“å­˜æ—¶é—´"""
        self.cache_tracker[cache_key] = datetime.now()
```

### 2.2 Token ä½¿ç”¨ä¼˜åŒ–

**ç­–ç•¥ 1ï¼šæ¸è¿›å¼ä¸Šä¸‹æ–‡åŠ è½½**

```python
def get_progressive_context(session_id: str, query: str) -> List[Dict]:
    """æ¸è¿›åŠ è½½ä¸Šä¸‹æ–‡"""
    context = []
    token_budget = 100_000
    used_tokens = 0
    
    # Level 1: æœ€è¿‘ 5 æ¡æ¶ˆæ¯ï¼ˆå¿…éœ€ï¼‰
    recent = get_recent_messages(session_id, limit=5)
    used_tokens += count_tokens(recent)
    context.extend(recent)
    
    # Level 2: ç›¸å…³çŸ¥è¯†ï¼ˆå¦‚æœæœ‰ç©ºé—´ï¼‰
    if used_tokens < token_budget * 0.5:
        knowledge = vector_search(query, top_k=3)
        knowledge_tokens = count_tokens(knowledge)
        if used_tokens + knowledge_tokens < token_budget:
            context.extend(knowledge)
            used_tokens += knowledge_tokens
    
    # Level 3: å®Œæ•´å†å²ï¼ˆå¦‚æœè¿˜æœ‰ç©ºé—´ï¼‰
    if used_tokens < token_budget * 0.7:
        full_history = get_all_messages(session_id)
        for msg in full_history:
            msg_tokens = count_tokens(msg)
            if used_tokens + msg_tokens < token_budget:
                context.append(msg)
                used_tokens += msg_tokens
            else:
                break
    
    logger.info(f"Context loaded: {used_tokens}/{token_budget} tokens")
    return context
```

**ç­–ç•¥ 2ï¼šæ™ºèƒ½æ‘˜è¦**

```python
async def summarize_old_messages(messages: List[Dict]) -> str:
    """å¼‚æ­¥æ‘˜è¦æ—§æ¶ˆæ¯"""
    if len(messages) < 20:
        return None  # å¤ªå°‘ä¸éœ€è¦æ‘˜è¦
    
    # åªæ‘˜è¦æ—§æ¶ˆæ¯ï¼ˆä¿ç•™æœ€è¿‘ 10 æ¡åŸæ–‡ï¼‰
    old_messages = messages[:-10]
    
    summary_prompt = f"""
    è¯·ç”¨ 500 tokens å†…æ€»ç»“ä»¥ä¸‹å¯¹è¯çš„å…³é”®ä¿¡æ¯ï¼š
    - ä¸»è¦è®¨è®ºçš„è¯é¢˜
    - é‡è¦çš„äº‹å®å’Œå†³ç­–
    - ç”¨æˆ·çš„åå¥½å’Œéœ€æ±‚
    
    å¯¹è¯å†…å®¹ï¼š
    {format_messages(old_messages)}
    """
    
    summary = await call_llm(summary_prompt, max_tokens=500)
    
    # å‹ç¼©ç‡
    old_tokens = sum(count_tokens(m['content']) for m in old_messages)
    new_tokens = count_tokens(summary)
    compression_ratio = old_tokens / new_tokens
    
    logger.info(f"Summarized {len(old_messages)} messages: "
                f"{old_tokens} -> {new_tokens} tokens "
                f"({compression_ratio:.1f}x compression)")
    
    return summary
```

### 2.3 æˆæœ¬ç›‘æ§

```python
class CostTracker:
    """æˆæœ¬è¿½è¸ªå™¨"""
    
    # Claude 3.5 Sonnet å®šä»·ï¼ˆ2024å¹´1æœˆï¼‰
    PRICING = {
        'input': 3.0,           # $3/M tokens
        'output': 15.0,         # $15/M tokens
        'cache_write': 3.75,    # $3.75/M tokens
        'cache_read': 0.30      # $0.30/M tokens
    }
    
    def __init__(self):
        self.usage_log = []
    
    def track(self, usage: Dict, session_id: str):
        """è¿½è¸ª API ä½¿ç”¨"""
        cost = {
            'session_id': session_id,
            'timestamp': datetime.now().isoformat(),
            'input_tokens': usage.get('input_tokens', 0),
            'output_tokens': usage.get('output_tokens', 0),
            'cache_read': usage.get('cache_read_input_tokens', 0),
            'cache_write': usage.get('cache_creation_input_tokens', 0)
        }
        
        # è®¡ç®—è´¹ç”¨
        cost['cost'] = (
            cost['input_tokens'] / 1_000_000 * self.PRICING['input'] +
            cost['output_tokens'] / 1_000_000 * self.PRICING['output'] +
            cost['cache_read'] / 1_000_000 * self.PRICING['cache_read'] +
            cost['cache_write'] / 1_000_000 * self.PRICING['cache_write']
        )
        
        self.usage_log.append(cost)
        
        # å®æ—¶å‘Šè­¦
        if cost['cost'] > 1.0:  # å•æ¬¡è¯·æ±‚è¶…è¿‡ $1
            logger.warning(f"âš ï¸  High cost request: ${cost['cost']:.2f}")
    
    def get_daily_cost(self, date: str = None) -> float:
        """è·å–æ¯æ—¥æˆæœ¬"""
        if date is None:
            date = datetime.now().date().isoformat()
        
        daily_logs = [
            log for log in self.usage_log
            if log['timestamp'].startswith(date)
        ]
        
        return sum(log['cost'] for log in daily_logs)
    
    def get_cache_hit_rate(self) -> float:
        """è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡"""
        total_input = sum(log['input_tokens'] for log in self.usage_log)
        cache_hits = sum(log['cache_read'] for log in self.usage_log)
        
        if total_input == 0:
            return 0.0
        
        return cache_hits / total_input
```

---

## 3. æ€§èƒ½ä¼˜åŒ–

### 3.1 å¹¶å‘å¤„ç†

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

class AsyncMemorySystem:
    """å¼‚æ­¥è®°å¿†ç³»ç»Ÿ"""
    
    def __init__(self):
        self.executor = ThreadPoolExecutor(max_workers=10)
    
    async def get_context_async(self, session_id: str, query: str) -> Dict:
        """å¹¶å‘è·å–ä¸Šä¸‹æ–‡"""
        # åŒæ—¶æ‰§è¡Œå¤šä¸ªæŸ¥è¯¢
        recent_task = asyncio.create_task(
            self._get_recent_async(session_id)
        )
        vector_task = asyncio.create_task(
            self._vector_search_async(query)
        )
        facts_task = asyncio.create_task(
            self._get_facts_async(session_id)
        )
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        recent, vectors, facts = await asyncio.gather(
            recent_task,
            vector_task,
            facts_task
        )
        
        return {
            'recent': recent,
            'vectors': vectors,
            'facts': facts
        }
    
    async def _get_recent_async(self, session_id: str):
        """å¼‚æ­¥è·å–æœ€è¿‘æ¶ˆæ¯"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            self.executor,
            self._get_recent_sync,
            session_id
        )
```

### 3.2 ç¼“å­˜ç­–ç•¥

```python
from functools import lru_cache
import hashlib

class SmartCache:
    """æ™ºèƒ½ç¼“å­˜å±‚"""
    
    def __init__(self):
        self.local_cache = {}  # è¿›ç¨‹å†…ç¼“å­˜
        self.redis = redis.Redis()  # åˆ†å¸ƒå¼ç¼“å­˜
    
    def get(self, key: str, fetcher: callable, ttl: int = 300):
        """å¤šçº§ç¼“å­˜è·å–"""
        # L1: è¿›ç¨‹å†…ç¼“å­˜ï¼ˆæœ€å¿«ï¼‰
        if key in self.local_cache:
            logger.debug(f"Cache hit (L1): {key}")
            return self.local_cache[key]
        
        # L2: Redis ç¼“å­˜
        cached = self.redis.get(key)
        if cached:
            logger.debug(f"Cache hit (L2): {key}")
            value = json.loads(cached)
            self.local_cache[key] = value  # å†™å…¥ L1
            return value
        
        # L3: ä»æºè·å–
        logger.debug(f"Cache miss: {key}")
        value = fetcher()
        
        # å†™å…¥æ‰€æœ‰ç¼“å­˜å±‚
        self.local_cache[key] = value
        self.redis.setex(key, ttl, json.dumps(value))
        
        return value
    
    @lru_cache(maxsize=1000)
    def get_embedding_cached(self, text: str) -> List[float]:
        """ç¼“å­˜ embeddingï¼ˆé¿å…é‡å¤è®¡ç®—ï¼‰"""
        text_hash = hashlib.md5(text.encode()).hexdigest()
        cache_key = f"emb:{text_hash}"
        
        return self.get(
            cache_key,
            lambda: self._compute_embedding(text),
            ttl=86400  # 24 å°æ—¶
        )
```

### 3.3 æ‰¹é‡å¤„ç†

```python
class BatchProcessor:
    """æ‰¹é‡å¤„ç†å™¨ï¼ˆæå‡ååé‡ï¼‰"""
    
    def __init__(self, batch_size: int = 10, flush_interval: float = 1.0):
        self.batch_size = batch_size
        self.flush_interval = flush_interval
        self.queue = []
        self.last_flush = time.time()
    
    async def add(self, item: Dict):
        """æ·»åŠ åˆ°æ‰¹æ¬¡é˜Ÿåˆ—"""
        self.queue.append(item)
        
        # è¾¾åˆ°æ‰¹æ¬¡å¤§å°æˆ–è¶…æ—¶ï¼Œè§¦å‘åˆ·æ–°
        should_flush = (
            len(self.queue) >= self.batch_size or
            time.time() - self.last_flush > self.flush_interval
        )
        
        if should_flush:
            await self.flush()
    
    async def flush(self):
        """æ‰¹é‡å¤„ç†é˜Ÿåˆ—"""
        if not self.queue:
            return
        
        batch = self.queue[:]
        self.queue = []
        self.last_flush = time.time()
        
        # æ‰¹é‡å‘é‡åŒ–
        texts = [item['text'] for item in batch]
        embeddings = await self._batch_embed(texts)
        
        # æ‰¹é‡æ’å…¥å‘é‡æ•°æ®åº“
        await self._batch_insert(batch, embeddings)
        
        logger.info(f"Flushed batch: {len(batch)} items")
    
    async def _batch_embed(self, texts: List[str]) -> List[List[float]]:
        """æ‰¹é‡ç”Ÿæˆ embedding"""
        # ä½¿ç”¨æ‰¹é‡ APIï¼ˆæ›´é«˜æ•ˆï¼‰
        return await embedding_service.embed_batch(texts)
```

---

## 4. ç›‘æ§ä¸å¯è§‚æµ‹æ€§

### 4.1 æ—¥å¿—ç³»ç»Ÿ

```python
import logging
import structlog

# ç»“æ„åŒ–æ—¥å¿—é…ç½®
structlog.configure(
    processors=[
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.JSONRenderer()
    ]
)

logger = structlog.get_logger()

class ObservableAgent:
    """å¯è§‚æµ‹çš„ Agent"""
    
    def chat(self, session_id: str, message: str) -> str:
        logger.info(
            "chat_started",
            session_id=session_id,
            message_length=len(message)
        )
        
        start_time = time.time()
        
        try:
            # è·å–ä¸Šä¸‹æ–‡
            context_start = time.time()
            context = self.memory.get_context(session_id, message)
            context_time = time.time() - context_start
            
            logger.info(
                "context_loaded",
                session_id=session_id,
                context_size=len(context),
                duration_ms=context_time * 1000
            )
            
            # è°ƒç”¨ LLM
            llm_start = time.time()
            response = self.llm.generate(context, message)
            llm_time = time.time() - llm_start
            
            logger.info(
                "llm_completed",
                session_id=session_id,
                input_tokens=response.usage.input_tokens,
                output_tokens=response.usage.output_tokens,
                duration_ms=llm_time * 1000
            )
            
            total_time = time.time() - start_time
            
            logger.info(
                "chat_completed",
                session_id=session_id,
                total_duration_ms=total_time * 1000
            )
            
            return response.content
            
        except Exception as e:
            logger.error(
                "chat_failed",
                session_id=session_id,
                error=str(e),
                duration_ms=(time.time() - start_time) * 1000
            )
            raise
```

### 4.2 æŒ‡æ ‡æ”¶é›†

```python
from prometheus_client import Counter, Histogram, Gauge

# Prometheus æŒ‡æ ‡
requests_total = Counter(
    'agent_requests_total',
    'Total number of agent requests',
    ['session_id', 'status']
)

request_duration = Histogram(
    'agent_request_duration_seconds',
    'Request duration',
    ['operation']
)

context_size = Histogram(
    'agent_context_size_tokens',
    'Context size in tokens'
)

cache_hit_rate = Gauge(
    'agent_cache_hit_rate',
    'Cache hit rate'
)

class MetricsAgent:
    """å¸¦æŒ‡æ ‡æ”¶é›†çš„ Agent"""
    
    @request_duration.labels(operation='chat').time()
    def chat(self, session_id: str, message: str) -> str:
        try:
            context = self.get_context(session_id, message)
            
            # è®°å½•ä¸Šä¸‹æ–‡å¤§å°
            context_tokens = count_tokens(context)
            context_size.observe(context_tokens)
            
            response = self.llm.generate(context, message)
            
            # è®°å½•æˆåŠŸè¯·æ±‚
            requests_total.labels(
                session_id=session_id,
                status='success'
            ).inc()
            
            # æ›´æ–°ç¼“å­˜å‘½ä¸­ç‡
            if hasattr(response.usage, 'cache_read_input_tokens'):
                hit_rate = (
                    response.usage.cache_read_input_tokens /
                    response.usage.input_tokens
                )
                cache_hit_rate.set(hit_rate)
            
            return response.content
            
        except Exception as e:
            requests_total.labels(
                session_id=session_id,
                status='error'
            ).inc()
            raise
```

### 4.3 å‘Šè­¦æœºåˆ¶

```python
class AlertManager:
    """å‘Šè­¦ç®¡ç†å™¨"""
    
    THRESHOLDS = {
        'high_cost': 10.0,          # å•æ—¥æˆæœ¬ > $10
        'low_cache_rate': 0.5,      # ç¼“å­˜å‘½ä¸­ç‡ < 50%
        'slow_response': 5.0,       # å“åº”æ—¶é—´ > 5s
        'high_token_usage': 150000  # å•æ¬¡è¯·æ±‚ > 150K tokens
    }
    
    def check_health(self, metrics: Dict) -> List[str]:
        """å¥åº·æ£€æŸ¥"""
        alerts = []
        
        # æ£€æŸ¥æˆæœ¬
        if metrics['daily_cost'] > self.THRESHOLDS['high_cost']:
            alerts.append(
                f"âš ï¸  Daily cost: ${metrics['daily_cost']:.2f} "
                f"(threshold: ${self.THRESHOLDS['high_cost']})"
            )
        
        # æ£€æŸ¥ç¼“å­˜å‘½ä¸­ç‡
        if metrics['cache_hit_rate'] < self.THRESHOLDS['low_cache_rate']:
            alerts.append(
                f"âš ï¸  Low cache hit rate: {metrics['cache_hit_rate']:.1%} "
                f"(threshold: {self.THRESHOLDS['low_cache_rate']:.1%})"
            )
        
        # æ£€æŸ¥å“åº”æ—¶é—´
        if metrics['avg_response_time'] > self.THRESHOLDS['slow_response']:
            alerts.append(
                f"âš ï¸  Slow response: {metrics['avg_response_time']:.2f}s "
                f"(threshold: {self.THRESHOLDS['slow_response']}s)"
            )
        
        return alerts
    
    def send_alerts(self, alerts: List[str]):
        """å‘é€å‘Šè­¦"""
        if not alerts:
            return
        
        # å‘é€åˆ° Slack/Email/PagerDuty
        message = "\n".join(alerts)
        logger.warning("health_check_alerts", alerts=message)
        
        # å®é™…å‘é€é€»è¾‘
        # slack_client.send_message(channel='#alerts', text=message)
```

---

## 5. é”™è¯¯å¤„ç†ä¸å®¹é”™

### 5.1 é‡è¯•æœºåˆ¶

```python
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type
)

class ResilientAgent:
    """å®¹é”™ Agent"""
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        retry=retry_if_exception_type(
            (requests.exceptions.Timeout, anthropic.APIError)
        )
    )
    def call_llm_with_retry(self, messages: List[Dict]) -> str:
        """å¸¦é‡è¯•çš„ LLM è°ƒç”¨"""
        try:
            response = self.client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=2048,
                messages=messages
            )
            return response.content[0].text
            
        except anthropic.RateLimitError as e:
            # é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾…åé‡è¯•
            wait_time = int(e.response.headers.get('retry-after', 60))
            logger.warning(f"Rate limited, waiting {wait_time}s")
            time.sleep(wait_time)
            raise
            
        except anthropic.APIError as e:
            logger.error(f"API error: {e}")
            raise
```

### 5.2 é™çº§ç­–ç•¥

```python
class FallbackAgent:
    """å¸¦é™çº§çš„ Agent"""
    
    def get_context(self, session_id: str, query: str) -> Dict:
        """å¤šçº§é™çº§è·å–ä¸Šä¸‹æ–‡"""
        try:
            # å°è¯•å®Œæ•´ç­–ç•¥ï¼ˆå‘é‡æœç´¢ + å®Œæ•´å†å²ï¼‰
            return self._get_full_context(session_id, query)
            
        except VectorDBError:
            logger.warning("Vector DB unavailable, using fallback")
            # é™çº§ 1ï¼šåªç”¨ session å†å²
            try:
                return self._get_session_context(session_id)
            except SessionError:
                logger.error("Session store unavailable, using minimal context")
                # é™çº§ 2ï¼šåªç”¨å½“å‰æŸ¥è¯¢
                return {"messages": [{"role": "user", "content": query}]}
    
    def chat(self, session_id: str, message: str) -> str:
        """å¸¦é™çº§çš„èŠå¤©"""
        try:
            # ä¸»æ¨¡å‹
            return self._call_claude(session_id, message)
            
        except anthropic.RateLimitError:
            # é™çº§åˆ°å¤‡ç”¨æ¨¡å‹
            logger.warning("Claude rate limited, using fallback model")
            return self._call_fallback_model(session_id, message)
            
        except Exception as e:
            # æœ€ç»ˆé™çº§ï¼šå›ºå®šå›å¤
            logger.error(f"All models failed: {e}")
            return "æŠ±æ­‰ï¼ŒæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•ã€‚"
```

### 5.3 æ•°æ®ä¸€è‡´æ€§

```python
class ConsistentMemory:
    """ä¿è¯ä¸€è‡´æ€§çš„è®°å¿†ç³»ç»Ÿ"""
    
    def save_message(self, session_id: str, message: Dict):
        """å¸¦äº‹åŠ¡çš„æ¶ˆæ¯ä¿å­˜"""
        transaction_id = str(uuid.uuid4())
        
        try:
            # 1. å†™å…¥ WAL (Write-Ahead Log)
            self._write_wal(transaction_id, session_id, message)
            
            # 2. å†™å…¥ä¸»å­˜å‚¨
            self._write_to_primary(session_id, message)
            
            # 3. å†™å…¥ç¼“å­˜
            self._write_to_cache(session_id, message)
            
            # 4. æ ‡è®°äº‹åŠ¡å®Œæˆ
            self._commit_transaction(transaction_id)
            
        except Exception as e:
            # å›æ»š
            logger.error(f"Save failed, rolling back: {e}")
            self._rollback_transaction(transaction_id)
            raise
    
    def recover(self):
        """ä» WAL æ¢å¤æœªå®Œæˆçš„äº‹åŠ¡"""
        incomplete_txs = self._get_incomplete_transactions()
        
        for tx in incomplete_txs:
            logger.info(f"Recovering transaction: {tx['id']}")
            try:
                self._replay_transaction(tx)
                self._commit_transaction(tx['id'])
            except Exception as e:
                logger.error(f"Recovery failed: {e}")
                self._rollback_transaction(tx['id'])
```

---

## 6. å®‰å…¨ä¸éšç§

### 6.1 æ•°æ®åŠ å¯†

```python
from cryptography.fernet import Fernet

class SecureMemory:
    """å®‰å…¨çš„è®°å¿†ç³»ç»Ÿ"""
    
    def __init__(self, encryption_key: bytes):
        self.cipher = Fernet(encryption_key)
    
    def save_message(self, session_id: str, message: Dict):
        """åŠ å¯†ä¿å­˜æ¶ˆæ¯"""
        # åºåˆ—åŒ–
        message_json = json.dumps(message)
        
        # åŠ å¯†
        encrypted = self.cipher.encrypt(message_json.encode())
        
        # ä¿å­˜åŠ å¯†æ•°æ®
        self._write_encrypted(session_id, encrypted)
    
    def load_messages(self, session_id: str) -> List[Dict]:
        """è§£å¯†åŠ è½½æ¶ˆæ¯"""
        # è¯»å–åŠ å¯†æ•°æ®
        encrypted = self._read_encrypted(session_id)
        
        # è§£å¯†
        decrypted = self.cipher.decrypt(encrypted)
        
        # ååºåˆ—åŒ–
        return json.loads(decrypted.decode())
```

### 6.2 æ•æ„Ÿä¿¡æ¯è„±æ•

```python
import re

class PrivacyFilter:
    """éšç§è¿‡æ»¤å™¨"""
    
    PATTERNS = {
        'email': r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
        'phone': r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b',
        'credit_card': r'\b\d{4}[-\s]?\d{4}[-\s]?\d{4}[-\s]?\d{4}\b',
        'ssn': r'\b\d{3}-\d{2}-\d{4}\b'
    }
    
    def redact(self, text: str) -> str:
        """è„±æ•å¤„ç†"""
        redacted = text
        
        for pii_type, pattern in self.PATTERNS.items():
            redacted = re.sub(
                pattern,
                f"[REDACTED_{pii_type.upper()}]",
                redacted
            )
        
        return redacted
    
    def filter_message(self, message: Dict) -> Dict:
        """è¿‡æ»¤æ¶ˆæ¯ä¸­çš„æ•æ„Ÿä¿¡æ¯"""
        filtered = message.copy()
        filtered['content'] = self.redact(message['content'])
        
        # ä¿å­˜åŸå§‹å†…å®¹çš„å“ˆå¸Œï¼ˆç”¨äºå®¡è®¡ï¼‰
        filtered['content_hash'] = hashlib.sha256(
            message['content'].encode()
        ).hexdigest()
        
        return filtered
```

---

## 7. å°ç»“

### ç”Ÿäº§ç¯å¢ƒæ£€æŸ¥æ¸…å•

- [ ] **æ¶æ„è®¾è®¡**
  - [ ] åˆ†å±‚è§£è€¦
  - [ ] å­˜å‚¨é€‰å‹åˆç†
  - [ ] æ”¯æŒæ°´å¹³æ‰©å±•

- [ ] **æˆæœ¬ä¼˜åŒ–**
  - [ ] Prompt Caching å…¨è¦†ç›–
  - [ ] Token ä½¿ç”¨ç›‘æ§
  - [ ] æˆæœ¬å‘Šè­¦æœºåˆ¶

- [ ] **æ€§èƒ½è°ƒä¼˜**
  - [ ] å¼‚æ­¥å¹¶å‘å¤„ç†
  - [ ] å¤šçº§ç¼“å­˜
  - [ ] æ‰¹é‡å¤„ç†

- [ ] **å¯è§‚æµ‹æ€§**
  - [ ] ç»“æ„åŒ–æ—¥å¿—
  - [ ] Metrics æ”¶é›†
  - [ ] å‘Šè­¦ç³»ç»Ÿ

- [ ] **å®¹é”™èƒ½åŠ›**
  - [ ] é‡è¯•æœºåˆ¶
  - [ ] é™çº§ç­–ç•¥
  - [ ] æ•°æ®ä¸€è‡´æ€§

- [ ] **å®‰å…¨åˆè§„**
  - [ ] æ•°æ®åŠ å¯†
  - [ ] æ•æ„Ÿä¿¡æ¯è„±æ•
  - [ ] è®¿é—®æ§åˆ¶

### æ¨èæŠ€æœ¯æ ˆ

| ç»„ä»¶ | å¼€å‘ç¯å¢ƒ | ç”Ÿäº§ç¯å¢ƒ |
|------|---------|---------|
| **Session** | JSON | Redis + PostgreSQL |
| **Vector DB** | ChromaDB | Qdrant / Pinecone |
| **ç›‘æ§** | Print | Prometheus + Grafana |
| **æ—¥å¿—** | Logging | Structlog + ELK |
| **éƒ¨ç½²** | æœ¬åœ° | Docker + K8s |

---

## ä¸‹ä¸€æ­¥

å¼€å§‹å®æˆ˜é¡¹ç›®ï¼Œå°†è¿™äº›æœ€ä½³å®è·µåº”ç”¨åˆ°å®é™…å¼€å‘ä¸­ï¼š
- **[Project 1: Session Manager](../projects/project_01_session_manager/)** - åŸºç¡€ä¼šè¯ç®¡ç†
- **[Project 2: Cached QA Bot](../projects/project_02_cached_qa_bot/)** - æˆæœ¬ä¼˜åŒ–å®æˆ˜
- **[Project 3: Vector Memory](../projects/project_03_vector_memory/)** - é•¿æœŸè®°å¿†ç³»ç»Ÿ
- **[Project 4: Context Compressor](../projects/project_04_context_compressor/)** - ä¸Šä¸‹æ–‡å‹ç¼©ä¼˜åŒ–
